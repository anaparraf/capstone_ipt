{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LjbK8ZEEfjx",
        "outputId": "e5de9e39-c506-4aff-e3a0-66d3a79f71af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.8.3)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.2.1)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.4)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.9.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio shapely scikit-image torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRckCc93HXtJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.plot import show\n",
        "from shapely.geometry import box\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W6ajkcCHaRo",
        "outputId": "c2838c03-2016-4cd9-8158-8220f1188d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8mlQOA0cmew"
      },
      "source": [
        "# Recorte de Rasters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Gadbw7cmex"
      },
      "outputs": [],
      "source": [
        "# def recortar_raster_por_coordenadas(input_raster, minx, miny, maxx, maxy, output_raster):\n",
        "#     \"\"\"\n",
        "#     Recorta um raster usando um ret√¢ngulo de coordenadas geogr√°ficas.\n",
        "#     \"\"\"\n",
        "#     bbox = box(minx, miny, maxx, maxy)\n",
        "#     geometries = [bbox]\n",
        "\n",
        "#     if not os.path.exists(input_raster):\n",
        "#         print(f\"Erro: Arquivo de entrada n√£o encontrado: {input_raster}\")\n",
        "#         return None\n",
        "\n",
        "#     try:\n",
        "#         with rasterio.open(input_raster) as src:\n",
        "#             out_image, out_transform = mask(src, geometries, crop=True)\n",
        "#             out_meta = src.meta.copy()\n",
        "\n",
        "#             out_meta.update({\n",
        "#                 \"driver\": \"GTiff\",\n",
        "#                 \"height\": out_image.shape[1],\n",
        "#                 \"width\": out_image.shape[2],\n",
        "#                 \"transform\": out_transform\n",
        "#             })\n",
        "\n",
        "#             with rasterio.open(output_raster, \"w\", **out_meta) as dest:\n",
        "#                 dest.write(out_image)\n",
        "\n",
        "#             print(f\"‚úÖ Recorte salvo em: {output_raster}\")\n",
        "#             print(f\"   Dimens√µes: {out_image.shape[1]} x {out_image.shape[2]}\")\n",
        "#             return output_raster\n",
        "\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Erro ao recortar o raster: {e}\")\n",
        "#         return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRgd6ghcmex"
      },
      "outputs": [],
      "source": [
        "# # Substitua pelos caminhos dos seus arquivos\n",
        "# anadem_path = \"ANADEM_AricanduvaBufferUTM.tif\"\n",
        "# geosampa_path = \"MDTGeosampa_AricanduvaBufferUTM.tif\"\n",
        "\n",
        "# # Coordenadas de recorte (exemplo)\n",
        "# minx_val, miny_val, maxx_val, maxy_val = 346000, 7391400, 347000, 7392400\n",
        "\n",
        "# rec_anadem = recortar_raster_por_coordenadas(anadem_path, minx_val, miny_val, maxx_val, maxy_val, \"recorte_anadem.tif\")\n",
        "# rec_geosampa = recortar_raster_por_coordenadas(geosampa_path, minx_val, miny_val, maxx_val, maxy_val, \"recorte_geosampa.tif\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lhzqL8Fcmey",
        "outputId": "46474d1d-5dc7-49c1-c3b6-e177d6f044e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (34, 34, 1)\n",
            "Total de pixels: 1156\n"
          ]
        }
      ],
      "source": [
        "# # numero dos pixels\n",
        "\n",
        "# with rasterio.open(\"anadem_reprojetado_recortado.tif\") as src:\n",
        "#     shape = (src.height, src.width, src.count)\n",
        "#     print(\"Shape:\", shape)\n",
        "#     total_pixels = src.height * src.width\n",
        "#     print(\"Total de pixels:\", total_pixels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OJfutfwcmez"
      },
      "source": [
        "#  Pr√©-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b-BSTwhcmez"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from rasterio.windows import Window\n",
        "\n",
        "class SuperResTiffDataset(Dataset):\n",
        "    def __init__(self, low_res_files, high_res_files, transform=None):\n",
        "        # The correct approach is to open each file in a loop and store the datasets.\n",
        "        self.low_res_datasets = [rasterio.open(f) for f in low_res_files]\n",
        "        self.high_res_datasets = [rasterio.open(f) for f in high_res_files]\n",
        "        self.transform = transform # This line is now correct\n",
        "\n",
        "        # Ensure the number of files is the same.\n",
        "        assert len(self.low_res_datasets) == len(self.high_res_datasets), \"Number of low-res and high-res files must match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return a large number of \"virtual\" examples for training.\n",
        "        # This is a good practice for patch-based datasets to ensure a diverse set of patches\n",
        "        # are used during each epoch.\n",
        "        return 10000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly select a file from the list of datasets for each item.\n",
        "        # This ensures we are not just iterating through files but drawing random patches from all files.\n",
        "        file_idx = np.random.randint(0, len(self.low_res_datasets))\n",
        "        low_res_src = self.low_res_datasets[file_idx]\n",
        "        high_res_src = self.high_res_datasets[file_idx]\n",
        "\n",
        "        # Define the window size for cropping. We must check the dimensions of the selected file.\n",
        "        patch_size = 32\n",
        "        max_x = low_res_src.width - patch_size\n",
        "        max_y = low_res_src.height - patch_size\n",
        "\n",
        "        if max_x < 0 or max_y < 0:\n",
        "            raise ValueError(f\"Image is too small for a {patch_size}x{patch_size} patch.\")\n",
        "\n",
        "        # Randomly select a starting corner for the window.\n",
        "        random_x = np.random.randint(0, max_x)\n",
        "        random_y = np.random.randint(0, max_y)\n",
        "        window = Window(random_x, random_y, patch_size, patch_size)\n",
        "\n",
        "        # Read the cropped images.\n",
        "        img_low = low_res_src.read(1, window=window).astype(np.float32)\n",
        "        img_high = high_res_src.read(1, window=window).astype(np.float32)\n",
        "\n",
        "        # ... (rest of the normalization and tensor conversion logic) ...\n",
        "\n",
        "        # L√≥gica de normaliza√ß√£o para img_low\n",
        "        nodata_low = low_res_src.nodata\n",
        "        valid_low_mask = img_low != nodata_low if nodata_low is not None else np.ones_like(img_low, dtype=bool)\n",
        "\n",
        "        if np.any(valid_low_mask):\n",
        "            min_low = np.min(img_low[valid_low_mask])\n",
        "            max_low = np.max(img_low[valid_low_mask])\n",
        "            if max_low - min_low > 0:\n",
        "                img_low_norm = np.zeros_like(img_low)\n",
        "                img_low_norm[valid_low_mask] = (img_low[valid_low_mask] - min_low) / (max_low - min_low)\n",
        "            else:\n",
        "                img_low_norm = np.zeros_like(img_low)\n",
        "        else:\n",
        "            img_low_norm = np.zeros_like(img_low)\n",
        "\n",
        "        # L√≥gica de normaliza√ß√£o para img_high\n",
        "        nodata_high = high_res_src.nodata\n",
        "        valid_high_mask = img_high != nodata_high if nodata_high is not None else np.ones_like(img_high, dtype=bool)\n",
        "\n",
        "        if np.any(valid_high_mask):\n",
        "            min_high = np.min(img_high[valid_high_mask])\n",
        "            max_high = np.max(img_high[valid_high_mask])\n",
        "            if max_high - min_high > 0:\n",
        "                img_high_norm = np.zeros_like(img_high)\n",
        "                img_high_norm[valid_high_mask] = (img_high[valid_high_mask] - min_high) / (max_high - min_high)\n",
        "            else:\n",
        "                img_high_norm = np.zeros_like(img_high)\n",
        "        else:\n",
        "            img_high_norm = np.zeros_like(img_high)\n",
        "\n",
        "        # Convert to PyTorch tensors and add a channel dimension.\n",
        "        low_res_tensor = torch.from_numpy(img_low_norm).unsqueeze(0)\n",
        "        high_res_tensor = torch.from_numpy(img_high_norm).unsqueeze(0)\n",
        "\n",
        "        # Apply optional transformations.\n",
        "        if self.transform:\n",
        "            low_res_tensor = self.transform(low_res_tensor)\n",
        "            high_res_tensor = self.transform(high_res_tensor)\n",
        "\n",
        "        return low_res_tensor, high_res_tensor\n",
        "\n",
        "    def __del__(self):\n",
        "        # Ensure all opened files are properly closed.\n",
        "        for dataset in self.low_res_datasets:\n",
        "            dataset.close()\n",
        "        for dataset in self.high_res_datasets:\n",
        "            dataset.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magN9rLBcme0"
      },
      "source": [
        "# Arquitetura U-Net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ2UZTqfcme1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_block(in_ch, out_ch):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class UNetFinal(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, base_filters=16):\n",
        "        super().__init__()\n",
        "        f = base_filters\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, f)\n",
        "        self.conv2 = conv_block(f, f*2)\n",
        "        self.conv3 = conv_block(f*2, f*4)\n",
        "        self.conv4 = conv_block(f*4, f*8)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bottleneck = conv_block(f*8, f*16)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)\n",
        "        self.conv_up4 = conv_block(f*16, f*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)\n",
        "        self.conv_up3 = conv_block(f*8, f*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)\n",
        "        self.conv_up2 = conv_block(f*4, f*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)\n",
        "        self.conv_up1 = conv_block(f*2, f)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(f, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.conv1(x); p1 = self.pool(c1)\n",
        "        c2 = self.conv2(p1); p2 = self.pool(c2)\n",
        "        c3 = self.conv3(p2); p3 = self.pool(c3)  # Linha corrigida\n",
        "        c4 = self.conv4(p3); p4 = self.pool(c4)\n",
        "        b = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder (com skip connections)\n",
        "        u4 = self.up4(b)\n",
        "        if u4.size() != c4.size():\n",
        "            u4 = F.interpolate(u4, size=c4.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u4 = self.conv_up4(torch.cat([u4, c4], 1))\n",
        "\n",
        "        u3 = self.up3(u4)\n",
        "        if u3.size() != c3.size():\n",
        "            u3 = F.interpolate(u3, size=c3.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u3 = self.conv_up3(torch.cat([u3, c3], 1))\n",
        "\n",
        "        u2 = self.up2(u3)\n",
        "        if u2.size() != c2.size():\n",
        "            u2 = F.interpolate(u2, size=c2.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u2 = self.conv_up2(torch.cat([u2, c2], 1))\n",
        "\n",
        "        u1 = self.up1(u2)\n",
        "        if u1.size() != c1.size():\n",
        "            u1 = F.interpolate(u1, size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u1 = self.conv_up1(torch.cat([u1, c1], 1))\n",
        "\n",
        "        return self.final_conv(u1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7IHWt95cme1"
      },
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtbnnrgedWN",
        "outputId": "3745eeac-337f-4eda-935b-816875e2efd7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando treinamento...\n"
          ]
        }
      ],
      "source": [
        "# Seus arquivos de entrada\n",
        "low_res_files = [\"rec_anadem.tif\"]\n",
        "high_res_files = [\"rec_geosampa.tif\"]\n",
        "\n",
        "dataset = SuperResTiffDataset(low_res_files, high_res_files)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# # Cria o dataset e o loader\n",
        "# dataset = SuperResTiffDataset(low_res_files, high_res_files)\n",
        "# loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Instancia o modelo, otimizador e a fun√ß√£o de perda\n",
        "model = UNetFinal().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# epochs = 50\n",
        "epochs = 70\n",
        "print(\"Iniciando treinamento...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for low_res_interp, high_res in loader:\n",
        "        low_res_interp, high_res = low_res_interp.to(device), high_res.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(low_res_interp)\n",
        "        loss = criterion(preds, high_res)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"√âpoca [{epoch+1}/{epochs}] | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "# Salva o modelo treinado\n",
        "torch.save(model.state_dict(), \"unet_superres.pth\")\n",
        "print(\"‚úÖ Treinamento conclu√≠do e modelo salvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8O-0-SYmJWn"
      },
      "source": [
        "# Inferencia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0dXb8wzc_Ke"
      },
      "outputs": [],
      "source": [
        "def gerar_superres_tif_independente(model, low_res_tif, out_path, device, target_size=(256*2, 256*2), scale_factor=60):\n",
        "    \"\"\"\n",
        "    Gera um raster de alta resolu√ß√£o a partir de um raster de baixa resolu√ß√£o usando um modelo treinado.\n",
        "    Args:\n",
        "        model: O modelo de rede neural para super-resolu√ß√£o.\n",
        "        low_res_tif: O caminho para o arquivo TIFF de baixa resolu√ß√£o de entrada.\n",
        "        out_path: O caminho para salvar o arquivo TIFF de alta resolu√ß√£o gerado.\n",
        "        device: O dispositivo (CPU ou GPU) onde o modelo ser√° executado.\n",
        "        target_size: O tamanho das sub-janelas a serem processadas.\n",
        "        scale_factor: O fator de escala para a super-resolu√ß√£o.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with rasterio.open(low_res_tif) as src:\n",
        "        # L√™ os dados de baixa resolu√ß√£o e o perfil\n",
        "        img_low = src.read(1).astype(np.float32)\n",
        "        profile = src.profile\n",
        "        nodata_val = src.nodata\n",
        "\n",
        "    # Identifica os pixels v√°lidos (ignorando os valores de NoData)\n",
        "    if nodata_val is not None:\n",
        "        valid_pixels_mask = img_low != nodata_val\n",
        "    else:\n",
        "        valid_pixels_mask = np.ones_like(img_low, dtype=bool)\n",
        "\n",
        "    # Calcula o m√≠nimo e o m√°ximo APENAS nos pixels v√°lidos\n",
        "    min_val = np.min(img_low[valid_pixels_mask])\n",
        "    max_val = np.max(img_low[valid_pixels_mask])\n",
        "\n",
        "    # Aplica a normaliza√ß√£o apenas nos pixels v√°lidos\n",
        "    img_low_norm = np.zeros_like(img_low)\n",
        "    if max_val - min_val != 0:\n",
        "        img_low_norm[valid_pixels_mask] = (img_low[valid_pixels_mask] - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Cria o tensor de entrada para o modelo\n",
        "    img_low_tensor = torch.from_numpy(img_low_norm).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Aplica o modelo\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(img_low_tensor)\n",
        "\n",
        "    # Converte a sa√≠da para um array numpy e remove as dimens√µes extras\n",
        "    output_array = output_tensor.cpu().numpy().squeeze()\n",
        "\n",
        "    # Desnormaliza a sa√≠da para a escala original dos dados\n",
        "    output_array = output_array * (max_val - min_val) + min_val\n",
        "\n",
        "    # Atualiza o perfil para o novo arquivo de alta resolu√ß√£o\n",
        "    profile.update(\n",
        "        height=profile['height'] * scale_factor,\n",
        "        width=profile['width'] * scale_factor,\n",
        "        transform=profile['transform'] * profile['transform'].scale(1/scale_factor, 1/scale_factor)\n",
        "    )\n",
        "\n",
        "    # Salva a imagem de alta resolu√ß√£o\n",
        "    with rasterio.open(out_path, 'w', **profile) as dst:\n",
        "        dst.write(output_array, 1)\n",
        "\n",
        "    print(f\"Arquivo gerado com sucesso: {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5UNj3UakevC",
        "outputId": "c84297c8-66e5-46ac-ab53-fb7ab7ba1490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo gerado com sucesso: rec_anadem_teste_UNET.tif\n"
          ]
        }
      ],
      "source": [
        "gerar_superres_tif_independente(model, \"rec_anadem_teste.tif\", \"rec_anadem_teste_UNET.tif\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrmJoKMcme9"
      },
      "source": [
        "# Avalia√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRTrHrSnejxV"
      },
      "outputs": [],
      "source": [
        "# --- AVALIA√á√ÉO E PLOTAGEM ---\n",
        "def avaliar_modelo(model, dataset, device):\n",
        "    model.eval()\n",
        "    mse_vals, psnr_vals, ssim_vals = [], [], []\n",
        "\n",
        "    for low_res_interp, high_res in dataset:\n",
        "        low_res_interp, high_res = low_res_interp.unsqueeze(0).to(device), high_res.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(low_res_interp).cpu().squeeze().numpy()\n",
        "\n",
        "        gt = high_res.cpu().squeeze().numpy()\n",
        "\n",
        "        # As m√©tricas esperam imagens 2D\n",
        "        gt = np.squeeze(gt)\n",
        "        pred = np.squeeze(pred)\n",
        "\n",
        "        mse_vals.append(mean_squared_error(gt, pred))\n",
        "        psnr_vals.append(peak_signal_noise_ratio(gt, pred, data_range=1))\n",
        "        ssim_vals.append(structural_similarity(gt, pred, data_range=1))\n",
        "\n",
        "    return {\n",
        "        \"MSE\": np.mean(mse_vals),\n",
        "        \"PSNR\": np.mean(psnr_vals),\n",
        "        \"SSIM\": np.mean(ssim_vals)\n",
        "    }\n",
        "\n",
        "metrics = avaliar_modelo(model, dataset, device)\n",
        "print(\"üìä M√©tricas de Avalia√ß√£o:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdmlazS9cme_"
      },
      "outputs": [],
      "source": [
        "def plotar_rasters(file_list):\n",
        "    \"\"\"\n",
        "    Plota rasters de uma lista de arquivos .tif com o nome no t√≠tulo.\n",
        "    \"\"\"\n",
        "    n = len(file_list)\n",
        "    fig, axs = plt.subplots(1, n, figsize=(5*n, 5))\n",
        "\n",
        "    if n == 1:\n",
        "        axs = [axs]  # garante que axs seja iter√°vel\n",
        "\n",
        "    for ax, file in zip(axs, file_list):\n",
        "        with rasterio.open(file) as src:\n",
        "            show(src, ax=ax, title=os.path.basename(file))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLqBkOvvcmfA"
      },
      "outputs": [],
      "source": [
        "plotar_rasters([\n",
        "    \"anadem_reprojetado_recortado.tif\",\n",
        "    \"geosampa_reprojetado_recortado.tif\",\n",
        "    \"novo_recortearicanduva_teste2.tif\",\n",
        "    \"saida_final.tif\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBFPWFC6cmfB"
      },
      "outputs": [],
      "source": [
        "def comparar_imagens(ref_path, pred_path):\n",
        "    \"\"\"\n",
        "    Compara imagem de refer√™ncia com predi√ß√£o do modelo.\n",
        "    Mostra as imagens lado a lado e imprime m√©tricas (MSE, PSNR, SSIM).\n",
        "    \"\"\"\n",
        "    # Carrega imagens\n",
        "    with rasterio.open(ref_path) as src:\n",
        "        ref = src.read(1).astype(np.float32)\n",
        "    with rasterio.open(pred_path) as src:\n",
        "        pred = src.read(1).astype(np.float32)\n",
        "\n",
        "    # Normaliza para [0,1] (necess√°rio para m√©tricas justas)\n",
        "    ref_norm = (ref - np.min(ref)) / (np.max(ref) - np.min(ref) + 1e-8)\n",
        "    pred_norm = (pred - np.min(pred)) / (np.max(pred) - np.min(pred) + 1e-8)\n",
        "\n",
        "    # Calcula m√©tricas\n",
        "    mse_val = mean_squared_error(ref_norm, pred_norm)\n",
        "    psnr_val = peak_signal_noise_ratio(ref_norm, pred_norm, data_range=1)\n",
        "    ssim_val = structural_similarity(ref_norm, pred_norm, data_range=1)\n",
        "\n",
        "    # Print m√©tricas\n",
        "    print(\"üìä M√©tricas:\")\n",
        "    print(f\"   MSE  = {mse_val:.6f}\")\n",
        "    print(f\"   PSNR = {psnr_val:.2f} dB\")\n",
        "    print(f\"   SSIM = {ssim_val:.4f}\")\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "    axs[0].imshow(ref, cmap='gray')\n",
        "    axs[0].set_title(\"Imagem Refer√™ncia\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    axs[1].imshow(pred, cmap='gray')\n",
        "    axs[1].set_title(f\"Super-Resolu√ß√£o\\nMSE={mse_val:.4f}, PSNR={psnr_val:.2f}, SSIM={ssim_val:.4f}\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\"MSE\": mse_val, \"PSNR\": psnr_val, \"SSIM\": ssim_val}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjkFB_dvmgLW"
      },
      "outputs": [],
      "source": [
        "# comparar_imagens(\"novo_recortearicanduva_teste2.tif\", \"saida_final_independente.tif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drs6YndjcmfC"
      },
      "source": [
        "# Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6WJACBZcmfD"
      },
      "outputs": [],
      "source": [
        "def benchmark(model, dataset, device):\n",
        "    start = time.time()\n",
        "    metrics = avaliar_modelo(model, dataset, device)\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"‚è± Tempo de avalia√ß√£o: {elapsed:.2f}s\")\n",
        "    return metrics, elapsed\n",
        "\n",
        "results, tempo = benchmark(model, dataset, device)\n",
        "print(\"üìä Resultados:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaiMEWJLzNgN"
      },
      "outputs": [],
      "source": [
        "model = UNetFinal().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26-vj3GzQZ2"
      },
      "source": [
        "Contagem Total\n",
        "Se somarmos todas as camadas de convolu√ß√£o (Conv2d) e convolu√ß√£o transposta (ConvTranspose2d), que s√£o as camadas \"trein√°veis\", o modelo tem:\n",
        "\n",
        "2 (conv1) + 2 (conv2) + 2 (conv3) + 2 (conv4) = 8 camadas de conv no encoder.\n",
        "\n",
        "2 (bottleneck) = 2 camadas de conv no gargalo.\n",
        "\n",
        "1 (up4) + 2 (conv_up4) + 1 (up3) + 2 (conv_up3) + 1 (up2) + 2 (conv_up2) + 1 (up1) + 2 (conv_up1) = 12 camadas no decoder.\n",
        "\n",
        "1 (final_conv) = 1 camada de conv na sa√≠da.\n",
        "\n",
        "O total √© de 8 + 2 + 12 + 1 = 23 camadas de convolu√ß√£o e convolu√ß√£o transposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww4qBKIq3wOY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # ‚û°Ô∏è 2. Set the path to your .pth file.\n",
        "# MODEL_PATH = 'unet_superres.pth'\n",
        "\n",
        "# # ‚û°Ô∏è 3. Check if the file exists.\n",
        "# if not os.path.exists(MODEL_PATH):\n",
        "#     print(f\"‚ùå Error: Model file not found at '{MODEL_PATH}'.\")\n",
        "# else:\n",
        "#     # ‚û°Ô∏è 4. Instantiate the model.\n",
        "#     # Decide if you want to use a GPU (cuda) or CPU\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = UNet().to(device)\n",
        "\n",
        "#     # ‚û°Ô∏è 5. Load the state dictionary from the .pth file.\n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "#         print(f\"‚úÖ Model successfully loaded from '{MODEL_PATH}'.\")\n",
        "\n",
        "#         # Now you can use the 'model' object for inference or evaluation.\n",
        "#         model.eval()\n",
        "#         # For example, to make a prediction:\n",
        "#         # dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
        "#         # output = model(dummy_input)\n",
        "#         # print(\"Model is ready for use.\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå An error occurred while loading the model: {e}\")\n",
        "#         print(\"This often happens if the model architecture doesn't match the saved weights.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}