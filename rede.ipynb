{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LjbK8ZEEfjx",
        "outputId": "e5de9e39-c506-4aff-e3a0-66d3a79f71af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.8.3)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.2.1)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.4)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.9.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio shapely scikit-image torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRckCc93HXtJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.plot import show\n",
        "from shapely.geometry import box\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W6ajkcCHaRo",
        "outputId": "c2838c03-2016-4cd9-8158-8220f1188d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8mlQOA0cmew"
      },
      "source": [
        "# Recorte de Rasters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Gadbw7cmex"
      },
      "outputs": [],
      "source": [
        "# def recortar_raster_por_coordenadas(input_raster, minx, miny, maxx, maxy, output_raster):\n",
        "#     \"\"\"\n",
        "#     Recorta um raster usando um retângulo de coordenadas geográficas.\n",
        "#     \"\"\"\n",
        "#     bbox = box(minx, miny, maxx, maxy)\n",
        "#     geometries = [bbox]\n",
        "\n",
        "#     if not os.path.exists(input_raster):\n",
        "#         print(f\"Erro: Arquivo de entrada não encontrado: {input_raster}\")\n",
        "#         return None\n",
        "\n",
        "#     try:\n",
        "#         with rasterio.open(input_raster) as src:\n",
        "#             out_image, out_transform = mask(src, geometries, crop=True)\n",
        "#             out_meta = src.meta.copy()\n",
        "\n",
        "#             out_meta.update({\n",
        "#                 \"driver\": \"GTiff\",\n",
        "#                 \"height\": out_image.shape[1],\n",
        "#                 \"width\": out_image.shape[2],\n",
        "#                 \"transform\": out_transform\n",
        "#             })\n",
        "\n",
        "#             with rasterio.open(output_raster, \"w\", **out_meta) as dest:\n",
        "#                 dest.write(out_image)\n",
        "\n",
        "#             print(f\"✅ Recorte salvo em: {output_raster}\")\n",
        "#             print(f\"   Dimensões: {out_image.shape[1]} x {out_image.shape[2]}\")\n",
        "#             return output_raster\n",
        "\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Erro ao recortar o raster: {e}\")\n",
        "#         return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRgd6ghcmex"
      },
      "outputs": [],
      "source": [
        "# # Substitua pelos caminhos dos seus arquivos\n",
        "# anadem_path = \"ANADEM_AricanduvaBufferUTM.tif\"\n",
        "# geosampa_path = \"MDTGeosampa_AricanduvaBufferUTM.tif\"\n",
        "\n",
        "# # Coordenadas de recorte (exemplo)\n",
        "# minx_val, miny_val, maxx_val, maxy_val = 346000, 7391400, 347000, 7392400\n",
        "\n",
        "# rec_anadem = recortar_raster_por_coordenadas(anadem_path, minx_val, miny_val, maxx_val, maxy_val, \"recorte_anadem.tif\")\n",
        "# rec_geosampa = recortar_raster_por_coordenadas(geosampa_path, minx_val, miny_val, maxx_val, maxy_val, \"recorte_geosampa.tif\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lhzqL8Fcmey",
        "outputId": "46474d1d-5dc7-49c1-c3b6-e177d6f044e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (34, 34, 1)\n",
            "Total de pixels: 1156\n"
          ]
        }
      ],
      "source": [
        "# # numero dos pixels\n",
        "\n",
        "# with rasterio.open(\"anadem_reprojetado_recortado.tif\") as src:\n",
        "#     shape = (src.height, src.width, src.count)\n",
        "#     print(\"Shape:\", shape)\n",
        "#     total_pixels = src.height * src.width\n",
        "#     print(\"Total de pixels:\", total_pixels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OJfutfwcmez"
      },
      "source": [
        "#  Pré-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b-BSTwhcmez"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from rasterio.windows import Window\n",
        "\n",
        "class SuperResTiffDataset(Dataset):\n",
        "    def __init__(self, low_res_files, high_res_files, transform=None):\n",
        "        # The correct approach is to open each file in a loop and store the datasets.\n",
        "        self.low_res_datasets = [rasterio.open(f) for f in low_res_files]\n",
        "        self.high_res_datasets = [rasterio.open(f) for f in high_res_files]\n",
        "        self.transform = transform # This line is now correct\n",
        "\n",
        "        # Ensure the number of files is the same.\n",
        "        assert len(self.low_res_datasets) == len(self.high_res_datasets), \"Number of low-res and high-res files must match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return a large number of \"virtual\" examples for training.\n",
        "        # This is a good practice for patch-based datasets to ensure a diverse set of patches\n",
        "        # are used during each epoch.\n",
        "        return 10000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly select a file from the list of datasets for each item.\n",
        "        # This ensures we are not just iterating through files but drawing random patches from all files.\n",
        "        file_idx = np.random.randint(0, len(self.low_res_datasets))\n",
        "        low_res_src = self.low_res_datasets[file_idx]\n",
        "        high_res_src = self.high_res_datasets[file_idx]\n",
        "\n",
        "        # Define the window size for cropping. We must check the dimensions of the selected file.\n",
        "        patch_size = 32\n",
        "        max_x = low_res_src.width - patch_size\n",
        "        max_y = low_res_src.height - patch_size\n",
        "\n",
        "        if max_x < 0 or max_y < 0:\n",
        "            raise ValueError(f\"Image is too small for a {patch_size}x{patch_size} patch.\")\n",
        "\n",
        "        # Randomly select a starting corner for the window.\n",
        "        random_x = np.random.randint(0, max_x)\n",
        "        random_y = np.random.randint(0, max_y)\n",
        "        window = Window(random_x, random_y, patch_size, patch_size)\n",
        "\n",
        "        # Read the cropped images.\n",
        "        img_low = low_res_src.read(1, window=window).astype(np.float32)\n",
        "        img_high = high_res_src.read(1, window=window).astype(np.float32)\n",
        "\n",
        "        # ... (rest of the normalization and tensor conversion logic) ...\n",
        "\n",
        "        # Lógica de normalização para img_low\n",
        "        nodata_low = low_res_src.nodata\n",
        "        valid_low_mask = img_low != nodata_low if nodata_low is not None else np.ones_like(img_low, dtype=bool)\n",
        "\n",
        "        if np.any(valid_low_mask):\n",
        "            min_low = np.min(img_low[valid_low_mask])\n",
        "            max_low = np.max(img_low[valid_low_mask])\n",
        "            if max_low - min_low > 0:\n",
        "                img_low_norm = np.zeros_like(img_low)\n",
        "                img_low_norm[valid_low_mask] = (img_low[valid_low_mask] - min_low) / (max_low - min_low)\n",
        "            else:\n",
        "                img_low_norm = np.zeros_like(img_low)\n",
        "        else:\n",
        "            img_low_norm = np.zeros_like(img_low)\n",
        "\n",
        "        # Lógica de normalização para img_high\n",
        "        nodata_high = high_res_src.nodata\n",
        "        valid_high_mask = img_high != nodata_high if nodata_high is not None else np.ones_like(img_high, dtype=bool)\n",
        "\n",
        "        if np.any(valid_high_mask):\n",
        "            min_high = np.min(img_high[valid_high_mask])\n",
        "            max_high = np.max(img_high[valid_high_mask])\n",
        "            if max_high - min_high > 0:\n",
        "                img_high_norm = np.zeros_like(img_high)\n",
        "                img_high_norm[valid_high_mask] = (img_high[valid_high_mask] - min_high) / (max_high - min_high)\n",
        "            else:\n",
        "                img_high_norm = np.zeros_like(img_high)\n",
        "        else:\n",
        "            img_high_norm = np.zeros_like(img_high)\n",
        "\n",
        "        # Convert to PyTorch tensors and add a channel dimension.\n",
        "        low_res_tensor = torch.from_numpy(img_low_norm).unsqueeze(0)\n",
        "        high_res_tensor = torch.from_numpy(img_high_norm).unsqueeze(0)\n",
        "\n",
        "        # Apply optional transformations.\n",
        "        if self.transform:\n",
        "            low_res_tensor = self.transform(low_res_tensor)\n",
        "            high_res_tensor = self.transform(high_res_tensor)\n",
        "\n",
        "        return low_res_tensor, high_res_tensor\n",
        "\n",
        "    def __del__(self):\n",
        "        # Ensure all opened files are properly closed.\n",
        "        for dataset in self.low_res_datasets:\n",
        "            dataset.close()\n",
        "        for dataset in self.high_res_datasets:\n",
        "            dataset.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magN9rLBcme0"
      },
      "source": [
        "# Arquitetura U-Net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ2UZTqfcme1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_block(in_ch, out_ch):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class UNetFinal(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, base_filters=16):\n",
        "        super().__init__()\n",
        "        f = base_filters\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, f)\n",
        "        self.conv2 = conv_block(f, f*2)\n",
        "        self.conv3 = conv_block(f*2, f*4)\n",
        "        self.conv4 = conv_block(f*4, f*8)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bottleneck = conv_block(f*8, f*16)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(f*16, f*8, kernel_size=2, stride=2)\n",
        "        self.conv_up4 = conv_block(f*16, f*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(f*8, f*4, kernel_size=2, stride=2)\n",
        "        self.conv_up3 = conv_block(f*8, f*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(f*4, f*2, kernel_size=2, stride=2)\n",
        "        self.conv_up2 = conv_block(f*4, f*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2)\n",
        "        self.conv_up1 = conv_block(f*2, f)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(f, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.conv1(x); p1 = self.pool(c1)\n",
        "        c2 = self.conv2(p1); p2 = self.pool(c2)\n",
        "        c3 = self.conv3(p2); p3 = self.pool(c3)  # Linha corrigida\n",
        "        c4 = self.conv4(p3); p4 = self.pool(c4)\n",
        "        b = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder (com skip connections)\n",
        "        u4 = self.up4(b)\n",
        "        if u4.size() != c4.size():\n",
        "            u4 = F.interpolate(u4, size=c4.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u4 = self.conv_up4(torch.cat([u4, c4], 1))\n",
        "\n",
        "        u3 = self.up3(u4)\n",
        "        if u3.size() != c3.size():\n",
        "            u3 = F.interpolate(u3, size=c3.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u3 = self.conv_up3(torch.cat([u3, c3], 1))\n",
        "\n",
        "        u2 = self.up2(u3)\n",
        "        if u2.size() != c2.size():\n",
        "            u2 = F.interpolate(u2, size=c2.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u2 = self.conv_up2(torch.cat([u2, c2], 1))\n",
        "\n",
        "        u1 = self.up1(u2)\n",
        "        if u1.size() != c1.size():\n",
        "            u1 = F.interpolate(u1, size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
        "        u1 = self.conv_up1(torch.cat([u1, c1], 1))\n",
        "\n",
        "        return self.final_conv(u1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7IHWt95cme1"
      },
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtbnnrgedWN",
        "outputId": "3745eeac-337f-4eda-935b-816875e2efd7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando treinamento...\n"
          ]
        }
      ],
      "source": [
        "# Seus arquivos de entrada\n",
        "low_res_files = [\"rec_anadem.tif\"]\n",
        "high_res_files = [\"rec_geosampa.tif\"]\n",
        "\n",
        "dataset = SuperResTiffDataset(low_res_files, high_res_files)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# # Cria o dataset e o loader\n",
        "# dataset = SuperResTiffDataset(low_res_files, high_res_files)\n",
        "# loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Instancia o modelo, otimizador e a função de perda\n",
        "model = UNetFinal().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# epochs = 50\n",
        "epochs = 70\n",
        "print(\"Iniciando treinamento...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for low_res_interp, high_res in loader:\n",
        "        low_res_interp, high_res = low_res_interp.to(device), high_res.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(low_res_interp)\n",
        "        loss = criterion(preds, high_res)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Época [{epoch+1}/{epochs}] | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "# Salva o modelo treinado\n",
        "torch.save(model.state_dict(), \"unet_superres.pth\")\n",
        "print(\"✅ Treinamento concluído e modelo salvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8O-0-SYmJWn"
      },
      "source": [
        "# Inferencia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0dXb8wzc_Ke"
      },
      "outputs": [],
      "source": [
        "def gerar_superres_tif_independente(model, low_res_tif, out_path, device, target_size=(256*2, 256*2), scale_factor=60):\n",
        "    \"\"\"\n",
        "    Gera um raster de alta resolução a partir de um raster de baixa resolução usando um modelo treinado.\n",
        "    Args:\n",
        "        model: O modelo de rede neural para super-resolução.\n",
        "        low_res_tif: O caminho para o arquivo TIFF de baixa resolução de entrada.\n",
        "        out_path: O caminho para salvar o arquivo TIFF de alta resolução gerado.\n",
        "        device: O dispositivo (CPU ou GPU) onde o modelo será executado.\n",
        "        target_size: O tamanho das sub-janelas a serem processadas.\n",
        "        scale_factor: O fator de escala para a super-resolução.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with rasterio.open(low_res_tif) as src:\n",
        "        # Lê os dados de baixa resolução e o perfil\n",
        "        img_low = src.read(1).astype(np.float32)\n",
        "        profile = src.profile\n",
        "        nodata_val = src.nodata\n",
        "\n",
        "    # Identifica os pixels válidos (ignorando os valores de NoData)\n",
        "    if nodata_val is not None:\n",
        "        valid_pixels_mask = img_low != nodata_val\n",
        "    else:\n",
        "        valid_pixels_mask = np.ones_like(img_low, dtype=bool)\n",
        "\n",
        "    # Calcula o mínimo e o máximo APENAS nos pixels válidos\n",
        "    min_val = np.min(img_low[valid_pixels_mask])\n",
        "    max_val = np.max(img_low[valid_pixels_mask])\n",
        "\n",
        "    # Aplica a normalização apenas nos pixels válidos\n",
        "    img_low_norm = np.zeros_like(img_low)\n",
        "    if max_val - min_val != 0:\n",
        "        img_low_norm[valid_pixels_mask] = (img_low[valid_pixels_mask] - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Cria o tensor de entrada para o modelo\n",
        "    img_low_tensor = torch.from_numpy(img_low_norm).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Aplica o modelo\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(img_low_tensor)\n",
        "\n",
        "    # Converte a saída para um array numpy e remove as dimensões extras\n",
        "    output_array = output_tensor.cpu().numpy().squeeze()\n",
        "\n",
        "    # Desnormaliza a saída para a escala original dos dados\n",
        "    output_array = output_array * (max_val - min_val) + min_val\n",
        "\n",
        "    # Atualiza o perfil para o novo arquivo de alta resolução\n",
        "    profile.update(\n",
        "        height=profile['height'] * scale_factor,\n",
        "        width=profile['width'] * scale_factor,\n",
        "        transform=profile['transform'] * profile['transform'].scale(1/scale_factor, 1/scale_factor)\n",
        "    )\n",
        "\n",
        "    # Salva a imagem de alta resolução\n",
        "    with rasterio.open(out_path, 'w', **profile) as dst:\n",
        "        dst.write(output_array, 1)\n",
        "\n",
        "    print(f\"Arquivo gerado com sucesso: {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5UNj3UakevC",
        "outputId": "c84297c8-66e5-46ac-ab53-fb7ab7ba1490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo gerado com sucesso: rec_anadem_teste_UNET.tif\n"
          ]
        }
      ],
      "source": [
        "gerar_superres_tif_independente(model, \"rec_anadem_teste.tif\", \"rec_anadem_teste_UNET.tif\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrmJoKMcme9"
      },
      "source": [
        "# Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRTrHrSnejxV"
      },
      "outputs": [],
      "source": [
        "# --- AVALIAÇÃO E PLOTAGEM ---\n",
        "def avaliar_modelo(model, dataset, device):\n",
        "    model.eval()\n",
        "    mse_vals, psnr_vals, ssim_vals = [], [], []\n",
        "\n",
        "    for low_res_interp, high_res in dataset:\n",
        "        low_res_interp, high_res = low_res_interp.unsqueeze(0).to(device), high_res.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(low_res_interp).cpu().squeeze().numpy()\n",
        "\n",
        "        gt = high_res.cpu().squeeze().numpy()\n",
        "\n",
        "        # As métricas esperam imagens 2D\n",
        "        gt = np.squeeze(gt)\n",
        "        pred = np.squeeze(pred)\n",
        "\n",
        "        mse_vals.append(mean_squared_error(gt, pred))\n",
        "        psnr_vals.append(peak_signal_noise_ratio(gt, pred, data_range=1))\n",
        "        ssim_vals.append(structural_similarity(gt, pred, data_range=1))\n",
        "\n",
        "    return {\n",
        "        \"MSE\": np.mean(mse_vals),\n",
        "        \"PSNR\": np.mean(psnr_vals),\n",
        "        \"SSIM\": np.mean(ssim_vals)\n",
        "    }\n",
        "\n",
        "metrics = avaliar_modelo(model, dataset, device)\n",
        "print(\"📊 Métricas de Avaliação:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdmlazS9cme_"
      },
      "outputs": [],
      "source": [
        "def plotar_rasters(file_list):\n",
        "    \"\"\"\n",
        "    Plota rasters de uma lista de arquivos .tif com o nome no título.\n",
        "    \"\"\"\n",
        "    n = len(file_list)\n",
        "    fig, axs = plt.subplots(1, n, figsize=(5*n, 5))\n",
        "\n",
        "    if n == 1:\n",
        "        axs = [axs]  # garante que axs seja iterável\n",
        "\n",
        "    for ax, file in zip(axs, file_list):\n",
        "        with rasterio.open(file) as src:\n",
        "            show(src, ax=ax, title=os.path.basename(file))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLqBkOvvcmfA"
      },
      "outputs": [],
      "source": [
        "plotar_rasters([\n",
        "    \"anadem_reprojetado_recortado.tif\",\n",
        "    \"geosampa_reprojetado_recortado.tif\",\n",
        "    \"novo_recortearicanduva_teste2.tif\",\n",
        "    \"saida_final.tif\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBFPWFC6cmfB"
      },
      "outputs": [],
      "source": [
        "def comparar_imagens(ref_path, pred_path):\n",
        "    \"\"\"\n",
        "    Compara imagem de referência com predição do modelo.\n",
        "    Mostra as imagens lado a lado e imprime métricas (MSE, PSNR, SSIM).\n",
        "    \"\"\"\n",
        "    # Carrega imagens\n",
        "    with rasterio.open(ref_path) as src:\n",
        "        ref = src.read(1).astype(np.float32)\n",
        "    with rasterio.open(pred_path) as src:\n",
        "        pred = src.read(1).astype(np.float32)\n",
        "\n",
        "    # Normaliza para [0,1] (necessário para métricas justas)\n",
        "    ref_norm = (ref - np.min(ref)) / (np.max(ref) - np.min(ref) + 1e-8)\n",
        "    pred_norm = (pred - np.min(pred)) / (np.max(pred) - np.min(pred) + 1e-8)\n",
        "\n",
        "    # Calcula métricas\n",
        "    mse_val = mean_squared_error(ref_norm, pred_norm)\n",
        "    psnr_val = peak_signal_noise_ratio(ref_norm, pred_norm, data_range=1)\n",
        "    ssim_val = structural_similarity(ref_norm, pred_norm, data_range=1)\n",
        "\n",
        "    # Print métricas\n",
        "    print(\"📊 Métricas:\")\n",
        "    print(f\"   MSE  = {mse_val:.6f}\")\n",
        "    print(f\"   PSNR = {psnr_val:.2f} dB\")\n",
        "    print(f\"   SSIM = {ssim_val:.4f}\")\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "    axs[0].imshow(ref, cmap='gray')\n",
        "    axs[0].set_title(\"Imagem Referência\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    axs[1].imshow(pred, cmap='gray')\n",
        "    axs[1].set_title(f\"Super-Resolução\\nMSE={mse_val:.4f}, PSNR={psnr_val:.2f}, SSIM={ssim_val:.4f}\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\"MSE\": mse_val, \"PSNR\": psnr_val, \"SSIM\": ssim_val}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjkFB_dvmgLW"
      },
      "outputs": [],
      "source": [
        "# comparar_imagens(\"novo_recortearicanduva_teste2.tif\", \"saida_final_independente.tif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drs6YndjcmfC"
      },
      "source": [
        "# Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6WJACBZcmfD"
      },
      "outputs": [],
      "source": [
        "def benchmark(model, dataset, device):\n",
        "    start = time.time()\n",
        "    metrics = avaliar_modelo(model, dataset, device)\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"⏱ Tempo de avaliação: {elapsed:.2f}s\")\n",
        "    return metrics, elapsed\n",
        "\n",
        "results, tempo = benchmark(model, dataset, device)\n",
        "print(\"📊 Resultados:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaiMEWJLzNgN"
      },
      "outputs": [],
      "source": [
        "model = UNetFinal().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26-vj3GzQZ2"
      },
      "source": [
        "Contagem Total\n",
        "Se somarmos todas as camadas de convolução (Conv2d) e convolução transposta (ConvTranspose2d), que são as camadas \"treináveis\", o modelo tem:\n",
        "\n",
        "2 (conv1) + 2 (conv2) + 2 (conv3) + 2 (conv4) = 8 camadas de conv no encoder.\n",
        "\n",
        "2 (bottleneck) = 2 camadas de conv no gargalo.\n",
        "\n",
        "1 (up4) + 2 (conv_up4) + 1 (up3) + 2 (conv_up3) + 1 (up2) + 2 (conv_up2) + 1 (up1) + 2 (conv_up1) = 12 camadas no decoder.\n",
        "\n",
        "1 (final_conv) = 1 camada de conv na saída.\n",
        "\n",
        "O total é de 8 + 2 + 12 + 1 = 23 camadas de convolução e convolução transposta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww4qBKIq3wOY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # ➡️ 2. Set the path to your .pth file.\n",
        "# MODEL_PATH = 'unet_superres.pth'\n",
        "\n",
        "# # ➡️ 3. Check if the file exists.\n",
        "# if not os.path.exists(MODEL_PATH):\n",
        "#     print(f\"❌ Error: Model file not found at '{MODEL_PATH}'.\")\n",
        "# else:\n",
        "#     # ➡️ 4. Instantiate the model.\n",
        "#     # Decide if you want to use a GPU (cuda) or CPU\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = UNet().to(device)\n",
        "\n",
        "#     # ➡️ 5. Load the state dictionary from the .pth file.\n",
        "#     try:\n",
        "#         model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "#         print(f\"✅ Model successfully loaded from '{MODEL_PATH}'.\")\n",
        "\n",
        "#         # Now you can use the 'model' object for inference or evaluation.\n",
        "#         model.eval()\n",
        "#         # For example, to make a prediction:\n",
        "#         # dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
        "#         # output = model(dummy_input)\n",
        "#         # print(\"Model is ready for use.\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ An error occurred while loading the model: {e}\")\n",
        "#         print(\"This often happens if the model architecture doesn't match the saved weights.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}