"""
U-Net Cl√°ssica com 2 Entradas (LIDAR + ANADEM) ‚Üí 1 Sa√≠da (target refinado)
Todos os dados est√£o na MESMA resolu√ß√£o (5m).

Estrutura:
- Entrada: 2 canais (LIDAR reamostrado 0.5m‚Üí5m + ANADEM reamostrado 30m‚Üí5m)
- Sa√≠da: 1 canal (vers√£o refinada em 5m)
- U-Net sim√©trica: 4 n√≠veis encoder + bottleneck + 4 n√≠veis decoder
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

import rasterio
from rasterio.windows import Window

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# ---------------------------
# Configura√ß√£o
# ---------------------------
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.benchmark = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Dispositivo: {device}")

# ---------------------------
# M√©tricas (PSNR mask-aware)
# ---------------------------
def calculate_psnr_masked(pred, target, mask, max_val=1.0):
    """PSNR considerando apenas pixels v√°lidos (mask=1)"""
    B = pred.size(0)
    psnrs = []
    for b in range(B):
        m = mask[b:b+1].bool()
        if m.sum() == 0:
            continue
        p = pred[b:b+1][m]
        t = target[b:b+1][m]
        mse = torch.mean((p - t) ** 2)
        if mse == 0:
            psnrs.append(100.0)
        else:
            psnrs.append((20 * torch.log10(torch.tensor(max_val) / torch.sqrt(mse))).item())
    return float(np.mean(psnrs)) if psnrs else float('nan')

# ---------------------------
# Logger
# ---------------------------
class TrainingLogger:
    def __init__(self, log_dir="logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True, parents=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"training_{self.timestamp}.json"
        self.data = {
            'timestamp': self.timestamp,
            'config': {},
            'epochs': [],
            'metrics': {
                'train_loss': [], 'val_loss': [],
                'train_psnr': [], 'val_psnr': [],
                'learning_rate': []
            }
        }

    def set_config(self, config):
        self.data['config'] = config

    def log_epoch(self, epoch, train_loss, val_loss, train_psnr, val_psnr, lr):
        self.data['epochs'].append({
            'epoch': epoch, 'train_loss': float(train_loss),
            'val_loss': float(val_loss) if val_loss else None,
            'train_psnr': float(train_psnr) if train_psnr else None,
            'val_psnr': float(val_psnr) if val_psnr else None,
            'lr': float(lr)
        })
        self.data['metrics']['train_loss'].append(float(train_loss))
        self.data['metrics']['val_loss'].append(float(val_loss) if val_loss else float('nan'))
        self.data['metrics']['train_psnr'].append(float(train_psnr) if train_psnr else float('nan'))
        self.data['metrics']['val_psnr'].append(float(val_psnr) if val_psnr else float('nan'))
        self.data['metrics']['learning_rate'].append(float(lr))
        self._save()

    def _save(self):
        with open(self.log_file, 'w') as f:
            json.dump(self.data, f, indent=2)

    def get_log_file(self):
        return str(self.log_file)

class TrainingVisualizer:
    @staticmethod
    def plot_curves(log_file, out_png=None):
        with open(log_file, 'r') as f:
            data = json.load(f)
        metrics = data['metrics']
        epochs = np.arange(1, len(metrics['train_loss']) + 1)

        fig = plt.figure(figsize=(14, 5))
        
        ax1 = plt.subplot(1, 2, 1)
        ax1.plot(epochs, metrics['train_loss'], 'b-', label='Train Loss', linewidth=2)
        ax1.plot(epochs, metrics['val_loss'], 'r-', label='Val Loss', linewidth=2)
        ax1.set_xlabel('√âpoca', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_title('Loss de Treinamento')

        ax2 = plt.subplot(1, 2, 2)
        ax2.plot(epochs, metrics['train_psnr'], 'b-', label='Train PSNR', linewidth=2)
        ax2.plot(epochs, metrics['val_psnr'], 'r-', label='Val PSNR', linewidth=2)
        ax2.set_xlabel('√âpoca', fontsize=12)
        ax2.set_ylabel('PSNR (dB)', fontsize=12)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_title('PSNR de Valida√ß√£o')

        plt.tight_layout()
        if out_png is None:
            out_png = log_file.replace('.json', '_curves.png')
        plt.savefig(out_png, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"Gr√°ficos salvos: {out_png}")

# ---------------------------
# Dataset: 2 Entradas + 1 Target
# ---------------------------
class DualInputDataset(Dataset):
    """
    Carrega 3 rasters (todos na mesma resolu√ß√£o 5m):
    - anadem_files: ANADEM reamostrado 30m‚Üí5m (entrada 1)
    - lidar_files: LIDAR reamostrado 0.5m‚Üí5m (entrada 2)
    - target_files: Ground truth em 5m (sa√≠da esperada)
    """
    def __init__(self, anadem_files, lidar_files, target_files, 
                 patch_size=128, samples_per_file=1000):
        assert len(anadem_files) == len(lidar_files) == len(target_files)
        self.anadem = [rasterio.open(p) for p in anadem_files]
        self.lidar = [rasterio.open(p) for p in lidar_files]
        self.target = [rasterio.open(p) for p in target_files]
        self.patch_size = patch_size
        self.samples_per_file = samples_per_file
        
        print(f"Dataset inicializado com {len(anadem_files)} conjuntos de imagens")

    def __len__(self):
        return len(self.anadem) * self.samples_per_file

    def _normalize(self, img, nodata):
        """Normaliza para [0,1] usando percentis 2-98"""
        valid = np.isfinite(img)
        if nodata is not None:
            valid &= (img != nodata)
        
        if not np.any(valid):
            return np.zeros_like(img, dtype=np.float32), valid.astype(np.uint8)
        
        arr = img.copy().astype(np.float32)
        vals = arr[valid]
        p2, p98 = np.percentile(vals, [2, 98])
        
        if (p98 - p2) < 1e-6:
            p98 = p2 + 1e-6
            
        norm = np.zeros_like(arr, dtype=np.float32)
        norm[valid] = np.clip((arr[valid] - p2) / (p98 - p2), 0, 1)
        
        return norm, valid.astype(np.uint8)

    def __getitem__(self, idx):
        file_idx = idx % len(self.anadem)
        
        # Abre os 3 rasters
        anadem_src = self.anadem[file_idx]
        lidar_src = self.lidar[file_idx]
        target_src = self.target[file_idx]
        
        # Encontra dimens√µes m√≠nimas entre os 3 rasters
        min_width = min(anadem_src.width, lidar_src.width, target_src.width)
        min_height = min(anadem_src.height, lidar_src.height, target_src.height)
        
        # Extrai patch aleat√≥rio (garante que cabe em todos os 3)
        max_x = max(0, min_width - self.patch_size)
        max_y = max(0, min_height - self.patch_size)
        
        if max_x == 0 or max_y == 0:
            # Se rasters s√£o menores que patch_size, usa tamanho dispon√≠vel
            rx, ry = 0, 0
            patch_w = min(min_width, self.patch_size)
            patch_h = min(min_height, self.patch_size)
        else:
            rx = np.random.randint(0, max_x + 1)
            ry = np.random.randint(0, max_y + 1)
            patch_w = self.patch_size
            patch_h = self.patch_size
        
        # Mesma janela para os 3 rasters
        window = Window(rx, ry, patch_w, patch_h)
        
        # L√™ dados
        try:
            anadem = anadem_src.read(1, window=window).astype(np.float32)
            lidar = lidar_src.read(1, window=window).astype(np.float32)
            target = target_src.read(1, window=window).astype(np.float32)
        except Exception as e:
            print(f"[WARN] Erro ao ler window {window}: {e}")
            # Fallback: l√™ regi√£o top-left
            anadem = anadem_src.read(1).astype(np.float32)[:self.patch_size, :self.patch_size]
            lidar = lidar_src.read(1).astype(np.float32)[:self.patch_size, :self.patch_size]
            target = target_src.read(1).astype(np.float32)[:self.patch_size, :self.patch_size]
        
        # CR√çTICO: Garante que todos t√™m EXATAMENTE o mesmo shape
        # Se tiverem shapes diferentes, resample para o menor comum
        shapes = [anadem.shape, lidar.shape, target.shape]
        if len(set(shapes)) > 1:
            print(f"[WARN] Shapes diferentes detectados: {shapes}")
            # Encontra menor shape comum
            min_h = min(s[0] for s in shapes)
            min_w = min(s[1] for s in shapes)
            
            # Crop todos para mesmo tamanho
            anadem = anadem[:min_h, :min_w]
            lidar = lidar[:min_h, :min_w]
            target = target[:min_h, :min_w]
        
        # Padding se necess√°rio (garante patch_size x patch_size)
        if anadem.shape[0] < self.patch_size or anadem.shape[1] < self.patch_size:
            anadem = np.pad(anadem, 
                          ((0, max(0, self.patch_size - anadem.shape[0])),
                           (0, max(0, self.patch_size - anadem.shape[1]))),
                          mode='constant', constant_values=0)
            lidar = np.pad(lidar,
                          ((0, max(0, self.patch_size - lidar.shape[0])),
                           (0, max(0, self.patch_size - lidar.shape[1]))),
                          mode='constant', constant_values=0)
            target = np.pad(target,
                           ((0, max(0, self.patch_size - target.shape[0])),
                            (0, max(0, self.patch_size - target.shape[1]))),
                           mode='constant', constant_values=0)
        
        # Normaliza
        anadem_n, mask_a = self._normalize(anadem, anadem_src.nodata)
        lidar_n, mask_l = self._normalize(lidar, lidar_src.nodata)
        target_n, mask_t = self._normalize(target, target_src.nodata)
        
        # Verifica shapes das masks ANTES de combinar
        assert mask_a.shape == mask_l.shape == mask_t.shape, \
            f"Mask shapes n√£o batem: {mask_a.shape}, {mask_l.shape}, {mask_t.shape}"
        
        # Mask final: interse√ß√£o dos 3
        mask = (mask_a & mask_l & mask_t).astype(np.float32)
        
        # Converte para tensores: (1,H,W) cada
        anadem_t = torch.from_numpy(anadem_n).unsqueeze(0)
        lidar_t = torch.from_numpy(lidar_n).unsqueeze(0)
        target_t = torch.from_numpy(target_n).unsqueeze(0)
        mask_t = torch.from_numpy(mask).unsqueeze(0)
        
        # Concatena as 2 entradas: (2,H,W)
        inputs = torch.cat([anadem_t, lidar_t], dim=0)
        
        return inputs, target_t, mask_t

    def __del__(self):
        for src in self.anadem + self.lidar + self.target:
            try:
                src.close()
            except:
                pass

# ---------------------------
# Modelo: U-Net Cl√°ssica Sim√©trica
# ---------------------------
class DoubleConv(nn.Module):
    """Bloco cl√°ssico da U-Net: Conv-BN-ReLU-Conv-BN-ReLU"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    """
    U-Net Cl√°ssica Sim√©trica
    - in_channels: n√∫mero de canais de entrada (2 para ANADEM+LIDAR)
    - out_channels: n√∫mero de canais de sa√≠da (1 para target)
    - features: lista de canais em cada n√≠vel [64, 128, 256, 512]
    """
    def __init__(self, in_channels=2, out_channels=1, features=[64, 128, 256, 512]):
        super().__init__()
        self.downs = nn.ModuleList()
        self.ups = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ENCODER (Downsampling)
        in_ch = in_channels
        for feature in features:
            self.downs.append(DoubleConv(in_ch, feature))
            in_ch = feature
        
        # BOTTLENECK
        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)
        
        # DECODER (Upsampling)
        for feature in reversed(features):
            # Transposed conv para upsampling
            self.ups.append(
                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)
            )
            # DoubleConv ap√≥s concatena√ß√£o com skip connection
            # Entrada: feature (up) + feature (skip) = feature * 2
            self.ups.append(DoubleConv(feature * 2, feature))
        
        # Camada final: 1x1 conv
        self.final = nn.Conv2d(features[0], out_channels, kernel_size=1)
        
        # Inicializa√ß√£o He
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        skip_connections = []
        
        # ENCODER
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = self.pool(x)
        
        # BOTTLENECK
        x = self.bottleneck(x)
        
        # DECODER
        skip_connections = skip_connections[::-1]  # inverte ordem
        
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)  # upsampling
            skip = skip_connections[idx // 2]
            
            # Garante mesmo tamanho (por causa de padding)
            if x.shape != skip.shape:
                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)
            
            # Concatena com skip connection
            x = torch.cat([skip, x], dim=1)
            x = self.ups[idx + 1](x)  # double conv
        
        # Camada final
        return self.final(x)

# ---------------------------
# Loss Function (MSE + Gradient)
# ---------------------------
class CombinedLoss(nn.Module):
    """MSE + Gradient Loss, ambos mask-aware"""
    def __init__(self, alpha=0.7, beta=0.3):
        super().__init__()
        self.alpha = alpha  # peso do MSE
        self.beta = beta    # peso do gradiente
    
    def forward(self, pred, target, mask):
        # MSE mask-aware
        mask_count = torch.clamp(mask.sum(), min=1.0)
        mse = (((pred - target) ** 2) * mask).sum() / mask_count
        
        # Gradient loss mask-aware
        grad_loss = self._gradient_loss(pred, target, mask)
        
        return self.alpha * mse + self.beta * grad_loss
    
    def _gradient_loss(self, pred, target, mask):
        """Loss baseado em diferen√ßas de gradiente"""
        # Gradientes em x e y
        pred_gx = pred[:, :, :, 1:] - pred[:, :, :, :-1]
        pred_gy = pred[:, :, 1:, :] - pred[:, :, :-1, :]
        
        target_gx = target[:, :, :, 1:] - target[:, :, :, :-1]
        target_gy = target[:, :, 1:, :] - target[:, :, :-1, :]
        
        # Masks para gradientes
        mask_gx = mask[:, :, :, 1:] * mask[:, :, :, :-1]
        mask_gy = mask[:, :, 1:, :] * mask[:, :, :-1, :]
        
        # Loss
        loss_gx = (torch.abs(pred_gx - target_gx) * mask_gx).sum()
        loss_gy = (torch.abs(pred_gy - target_gy) * mask_gy).sum()
        
        count_gx = torch.clamp(mask_gx.sum(), min=1.0)
        count_gy = torch.clamp(mask_gy.sum(), min=1.0)
        
        return (loss_gx / count_gx + loss_gy / count_gy) / 2.0

# ---------------------------
# Valida√ß√£o de Dados
# ---------------------------
def validate_raster_compatibility(anadem_files, lidar_files, target_files):
    """Valida que os rasters s√£o compat√≠veis antes do treino"""
    print("\n" + "="*60)
    print("VALIDANDO COMPATIBILIDADE DOS RASTERS")
    print("="*60)
    
    for i, (a_path, l_path, t_path) in enumerate(zip(anadem_files, lidar_files, target_files)):
        print(f"\nConjunto {i+1}:")
        print(f"  ANADEM: {a_path}")
        print(f"  LIDAR:  {l_path}")
        print(f"  TARGET: {t_path}")
        
        with rasterio.open(a_path) as a_src, \
             rasterio.open(l_path) as l_src, \
             rasterio.open(t_path) as t_src:
            
            # Dimens√µes
            print(f"\n  Dimens√µes:")
            print(f"    ANADEM: {a_src.width} x {a_src.height}")
            print(f"    LIDAR:  {l_src.width} x {l_src.height}")
            print(f"    TARGET: {t_src.width} x {t_src.height}")
            
            # Resolu√ß√£o
            a_res = (abs(a_src.transform.a), abs(a_src.transform.e))
            l_res = (abs(l_src.transform.a), abs(l_src.transform.e))
            t_res = (abs(t_src.transform.a), abs(t_src.transform.e))
            
            print(f"\n  Resolu√ß√£o (x, y):")
            print(f"    ANADEM: {a_res[0]:.2f}m x {a_res[1]:.2f}m")
            print(f"    LIDAR:  {l_res[0]:.2f}m x {l_res[1]:.2f}m")
            print(f"    TARGET: {t_res[0]:.2f}m x {t_res[1]:.2f}m")
            
            # CRS
            print(f"\n  Sistema de Coordenadas:")
            print(f"    ANADEM: {a_src.crs}")
            print(f"    LIDAR:  {l_src.crs}")
            print(f"    TARGET: {t_src.crs}")
            
            # Warnings
            if a_src.crs != l_src.crs or a_src.crs != t_src.crs:
                print(f"  ‚ö†Ô∏è  AVISO: CRS diferentes! Considere reprojetar.")
            
            if not (4.5 <= a_res[0] <= 5.5 and 4.5 <= l_res[0] <= 5.5):
                print(f"  ‚ö†Ô∏è  AVISO: Resolu√ß√£o n√£o est√° pr√≥xima de 5m!")
            
            # Sobreposi√ß√£o espacial
            a_bounds = a_src.bounds
            l_bounds = l_src.bounds
            t_bounds = t_src.bounds
            
            def bounds_overlap(b1, b2):
                return not (b1.right < b2.left or b1.left > b2.right or
                          b1.top < b2.bottom or b1.bottom > b2.top)
            
            if not (bounds_overlap(a_bounds, l_bounds) and bounds_overlap(a_bounds, t_bounds)):
                print(f"  ‚ö†Ô∏è  AVISO: Rasters podem n√£o ter sobreposi√ß√£o espacial!")
    
    print("\n" + "="*60)
    print("Valida√ß√£o conclu√≠da. Verifique os avisos acima.")
    print("="*60 + "\n")

# ---------------------------
# Treinamento
# ---------------------------
def train_model(
    train_anadem, train_lidar, train_target,
    val_anadem, val_lidar, val_target,
    epochs=50, batch_size=4, patch_size=128,
    lr=1e-4, save_path="model/unet_dual_input.pth"
):
    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)
    
    logger = TrainingLogger()
    
    # Datasets
    train_ds = DualInputDataset(train_anadem, train_lidar, train_target, 
                                 patch_size=patch_size, samples_per_file=1000)
    val_ds = DualInputDataset(val_anadem, val_lidar, val_target,
                               patch_size=patch_size, samples_per_file=200)
    
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, 
                             num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,
                           num_workers=0, pin_memory=True)
    
    # Modelo
    model = UNet(in_channels=2, out_channels=1, features=[64, 128, 256, 512]).to(device)
    
    # Otimizador e scheduler
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )
    
    criterion = CombinedLoss(alpha=0.7, beta=0.3).to(device)
    
    config = {
        'epochs': epochs, 'batch_size': batch_size, 'patch_size': patch_size,
        'lr': lr, 'in_channels': 2, 'out_channels': 1
    }
    logger.set_config(config)
    
    best_val_psnr = -np.inf
    
    print(f"Iniciando treinamento: {epochs} √©pocas")
    print(f"Modelo: U-Net com 2 entradas (ANADEM+LIDAR) ‚Üí 1 sa√≠da")
    print("="*60)
    
    for epoch in range(1, epochs + 1):
        # TREINO
        model.train()
        train_loss = 0.0
        train_psnr = 0.0
        n_batches = 0
        
        for inputs, target, mask in train_loader:
            inputs = inputs.to(device)
            target = target.to(device)
            mask = mask.to(device)
            
            if mask.sum() < 1.0:
                continue
            
            optimizer.zero_grad()
            pred = model(inputs)
            loss = criterion(pred, target, mask)
            
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"[WARN] Epoch {epoch}: Loss inv√°lido, pulando batch")
                continue
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_psnr += calculate_psnr_masked(pred, target, mask)
            n_batches += 1
        
        avg_train_loss = train_loss / n_batches if n_batches > 0 else float('nan')
        avg_train_psnr = train_psnr / n_batches if n_batches > 0 else float('nan')
        
        # VALIDA√á√ÉO
        model.eval()
        val_loss = 0.0
        val_psnr = 0.0
        n_val = 0
        
        with torch.no_grad():
            for inputs, target, mask in val_loader:
                inputs = inputs.to(device)
                target = target.to(device)
                mask = mask.to(device)
                
                if mask.sum() < 1.0:
                    continue
                
                pred = model(inputs)
                loss = criterion(pred, target, mask)
                
                val_loss += loss.item()
                val_psnr += calculate_psnr_masked(pred, target, mask)
                n_val += 1
        
        avg_val_loss = val_loss / n_val if n_val > 0 else float('nan')
        avg_val_psnr = val_psnr / n_val if n_val > 0 else float('nan')
        
        # Scheduler
        scheduler.step(avg_val_loss if np.isfinite(avg_val_loss) else float('inf'))
        
        # Log
        lr_now = optimizer.param_groups[0]['lr']
        logger.log_epoch(epoch, avg_train_loss, avg_val_loss, 
                        avg_train_psnr, avg_val_psnr, lr_now)
        
        print(f"√âpoca {epoch}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | "
              f"Train PSNR: {avg_train_psnr:.2f} dB | Val PSNR: {avg_val_psnr:.2f} dB | "
              f"LR: {lr_now:.2e}")
        
        # Salva melhor modelo
        if np.isfinite(avg_val_psnr) and avg_val_psnr > best_val_psnr:
            best_val_psnr = avg_val_psnr
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_psnr': best_val_psnr,
                'config': config
            }, save_path)
            print(f"‚úì Melhor modelo salvo! PSNR: {best_val_psnr:.2f} dB")
    
    print("="*60)
    print("Treinamento finalizado!")
    return model, logger.get_log_file()

# ---------------------------
# Infer√™ncia
# ---------------------------
def generate_output(model_path, anadem_path, lidar_path, output_path, 
                   tile_size=256, overlap=0.5):
    """
    Gera raster de sa√≠da usando modelo treinado
    """
    # Carrega modelo
    ckpt = torch.load(model_path, map_location=device)
    model = UNet(in_channels=2, out_channels=1).to(device)
    model.load_state_dict(ckpt['model_state_dict'])
    model.eval()
    
    print(f"Modelo carregado: {model_path}")
    print(f"Val PSNR do checkpoint: {ckpt.get('val_psnr', 'N/A'):.2f} dB")
    
    # Abre rasters de entrada
    with rasterio.open(anadem_path) as anadem_src, \
         rasterio.open(lidar_path) as lidar_src:
        
        # L√™ e normaliza
        anadem = anadem_src.read(1).astype(np.float32)
        lidar = lidar_src.read(1).astype(np.float32)
        profile = anadem_src.profile.copy()
        
        # Normaliza√ß√£o simples (usar mesma l√≥gica do treino)
        def normalize(img, nodata):
            valid = np.isfinite(img)
            if nodata is not None:
                valid &= (img != nodata)
            if not np.any(valid):
                # retorna zeros + estat√≠sticas neutras
                return np.zeros_like(img, dtype=np.float32), 0.0, 1.0
            arr = img.copy().astype(np.float32)
            p2, p98 = np.percentile(arr[valid], [2, 98])
            if (p98 - p2) < 1e-6:
                p98 = p2 + 1e-6
            norm = np.zeros_like(arr, dtype=np.float32)
            norm[valid] = np.clip((arr[valid] - p2) / (p98 - p2), 0, 1)
            return norm, p2, p98
        
        anadem_n, a_p2, a_p98 = normalize(anadem, anadem_src.nodata)
        lidar_n, l_p2, l_p98 = normalize(lidar, lidar_src.nodata)
        
        # Prepara output
        h, w = anadem_n.shape
        output = np.zeros((h, w), dtype=np.float32)
        weight = np.zeros((h, w), dtype=np.float32)
        
        # Hanning window para blending suave
        hann = np.outer(np.hanning(tile_size), np.hanning(tile_size))
        
        # Processa em tiles
        step = int(tile_size * (1 - overlap))
        total_tiles = ((h - 1) // step + 1) * ((w - 1) // step + 1)
        processed = 0
        
        print(f"Processando {total_tiles} tiles...")
        
        for i in range(0, h, step):
            for j in range(0, w, step):
                i_end = min(i + tile_size, h)
                j_end = min(j + tile_size, w)
                
                # Extrai tiles (podem ter shapes diferentes entre os dois rasters)
                tile_a = anadem_n[i:i_end, j:j_end]
                tile_l = lidar_n[i:i_end, j:j_end]
                
                # Garante menor shape comum entre os dois canais (evita broadcast error)
                th_a, tw_a = tile_a.shape
                th_l, tw_l = tile_l.shape
                th = min(th_a, th_l)
                tw = min(tw_a, tw_l)
                
                # Crop para o menor shape comum
                tile_a = tile_a[:th, :tw]
                tile_l = tile_l[:th, :tw]
                
                # Padding se necess√°rio para atingir tile_size
                if th < tile_size or tw < tile_size:
                    pad_a = np.zeros((tile_size, tile_size), dtype=np.float32)
                    pad_l = np.zeros((tile_size, tile_size), dtype=np.float32)
                    pad_a[:th, :tw] = tile_a
                    pad_l[:th, :tw] = tile_l
                    tile_a, tile_l = pad_a, pad_l
                else:
                    # j√° tem tamanho completo
                    pass
                
                # Concatena canais: (2,H,W) -> (1,2,H,W)
                tile = np.stack([tile_a, tile_l], axis=0)
                tile_tensor = torch.from_numpy(tile).unsqueeze(0).to(device)
                
                # Infer√™ncia
                with torch.no_grad():
                    out_tile = model(tile_tensor).cpu().numpy().squeeze()
                
                # Usa apenas a regi√£o v√°lida (antes do padding)
                out_valid = out_tile[:th, :tw]
                w_valid = hann[:th, :tw]
                
                # Escreve na posi√ß√£o correta (usa th/tw reais)
                write_i_end = i + th
                write_j_end = j + tw
                output[i:write_i_end, j:write_j_end] += out_valid * w_valid
                weight[i:write_i_end, j:write_j_end] += w_valid
                
                processed += 1
                if processed % 100 == 0:
                    print(f"  {processed}/{total_tiles} tiles processados")
        
        # Normaliza√ß√£o final
        valid_w = weight > 1e-6
        output[valid_w] /= weight[valid_w]
        
        # Desnormaliza (usa estat√≠sticas do LIDAR como refer√™ncia)
        output = output * (l_p98 - l_p2) + l_p2
        
        # Salva
        profile.update(dtype='float32', count=1)
        os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(output.astype(np.float32), 1)
        
        print(f"‚úì Raster salvo: {output_path}")

# ---------------------------
# Execu√ß√£o
# ---------------------------
if __name__ == "__main__":
    # AJUSTE OS CAMINHOS DOS SEUS DADOS AQUI
    # =============================================
    # IMPORTANTE: Seus dados atuais
    # =============================================
    
    # Treino
    train_anadem_files = ["dados/anadem_5m.tif"]       # ANADEM reamostrado para 5m
    train_lidar_files = ["dados/geosampa_30m_reamostrado_5m.tif"]      # Usando GEOSAMPA como "LIDAR"
    train_target_files = ["dados/MDTGeosampa_AricanduvaBufferUTM.tif"]     # Ground truth em 5m
    
    # Valida√ß√£o (regi√£o diferente)
    val_anadem_files = ["dados/ANADEM_Recorte_IPT_5m.tif"]
    val_lidar_files = ["dados/GEOSAMPA_Recorte_IPT_reamostrado_5m.tif"]   # Target tamb√©m serve como input
    val_target_files = ["dados/GEOSAMPA_Recorte_IPT_reprojetado.tif"]
    
    # Cria diret√≥rios
    os.makedirs("model", exist_ok=True)
    os.makedirs("output", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    # VALIDA COMPATIBILIDADE DOS DADOS
    print("\nüîç Verificando dados de TREINO:")
    validate_raster_compatibility(train_anadem_files, train_lidar_files, train_target_files)
    
    print("\nüîç Verificando dados de VALIDA√á√ÉO:")
    validate_raster_compatibility(val_anadem_files, val_lidar_files, val_target_files)
    
    print("="*60)
    print("TREINAMENTO U-NET DUAL-INPUT")
    print("="*60)
    print("Entradas: 2 canais (ANADEM 5m + LIDAR 5m)")
    print("Sa√≠da: 1 canal (vers√£o refinada em 5m)")
    print("="*60)
    
    # Treina modelo
    model, log_file = train_model(
        train_anadem=train_anadem_files,
        train_lidar=train_lidar_files,
        train_target=train_target_files,
        val_anadem=val_anadem_files,
        val_lidar=val_lidar_files,
        val_target=val_target_files,
        epochs=90,           # Ajuste conforme necess√°rio
        batch_size=4,        # Ajuste conforme sua VRAM (4-8 √© seguro)
        patch_size=64,      # 128 ou 256
        lr=1e-4,             # Learning rate inicial
        save_path="model/unet_dual_input_best.pth"
    )
    
    print("\n" + "="*60)
    print("GERANDO GR√ÅFICOS DE TREINAMENTO")
    print("="*60)
    TrainingVisualizer.plot_curves(log_file)
    
    print("\n" + "="*60)
    print("INFER√äNCIA - GERANDO RASTER MELHORADO")
    print("="*60)
    
    # Verifica se modelo foi salvo
    model_path = "model/unet_dual_input_best.pth"
    if os.path.exists(model_path):
        # Gera output para regi√£o de valida√ß√£o
        generate_output(
            model_path=model_path,
            anadem_path="dados/ANADEM_Recorte_IPT_5m.tif",
            lidar_path="dados/GEOSAMPA_Recorte_IPT_reamostrado_5m.tif",
            output_path="output/resultado_unet_dual_input.tif",
            tile_size=256,
            overlap=0.5  # 50% overlap para blending suave
        )
        print("\n‚úì Pipeline completo finalizado!")
        print(f"  - Modelo: {model_path}")
        print(f"  - Logs: {log_file}")
        print(f"  - Output: output/resultado_unet_dual_input_90.tif")
    else:
        print(f"[ERRO] Modelo n√£o encontrado: {model_path}")
        print("Execute o treinamento primeiro.")
    
    print("="*60)
    print("FIM DA EXECU√á√ÉO")
    print("="*60)


# =============================================
# INSTRU√á√ïES DE USO
# =============================================
"""
1. PREPARA√á√ÉO DOS DADOS:
   Antes de rodar este script, voc√™ precisa reamostrar seus dados para 5m:
   
   a) ANADEM (30m ‚Üí 5m):
      gdal_translate -tr 5 5 -r bilinear anadem_30m.tif anadem_5m.tif
   
   b) LIDAR (0.5m ‚Üí 5m):
      gdal_translate -tr 5 5 -r average lidar_0.5m.tif lidar_5m.tif
   
   c) Target ground truth j√° deve estar em 5m (geosampa_5m.tif)

2. ESTRUTURA DE PASTAS:
   projeto/
   ‚îú‚îÄ‚îÄ dados/
   ‚îÇ   ‚îú‚îÄ‚îÄ anadem_5m.tif              # ANADEM reamostrado
   ‚îÇ   ‚îú‚îÄ‚îÄ lidar_5m.tif               # LIDAR reamostrado
   ‚îÇ   ‚îú‚îÄ‚îÄ geosampa_5m.tif            # Target (ground truth)
   ‚îÇ   ‚îú‚îÄ‚îÄ ANADEM_Recorte_IPT_5m.tif  # Valida√ß√£o
   ‚îÇ   ‚îú‚îÄ‚îÄ LIDAR_Recorte_IPT_5m.tif   # Valida√ß√£o
   ‚îÇ   ‚îî‚îÄ‚îÄ GEOSAMPA_Recorte_IPT.tif   # Valida√ß√£o target
   ‚îú‚îÄ‚îÄ model/                         # Checkpoints salvos aqui
   ‚îú‚îÄ‚îÄ output/                        # Rasters gerados aqui
   ‚îî‚îÄ‚îÄ logs/                          # JSONs de treinamento

3. AJUSTAR HIPERPAR√ÇMETROS:
   - batch_size: depende da VRAM (4-8 √© seguro, 16+ se tiver muita VRAM)
   - patch_size: 128 ou 256 (maior = mais contexto, mas mais VRAM)
   - epochs: 30-100 (ajuste conforme converg√™ncia)
   - lr: 1e-4 √© bom ponto de partida

4. MONITORAMENTO:
   - Logs salvos em logs/training_TIMESTAMP.json
   - Gr√°ficos salvos automaticamente como PNG
   - Modelo salvo quando val_psnr melhorar

5. INFER√äNCIA:
   Ap√≥s treinar, o script automaticamente gera o raster melhorado.
   Para gerar para outras regi√µes, use a fun√ß√£o generate_output()

6. TROUBLESHOOTING:
   - Se der OOM (out of memory): reduza batch_size ou patch_size
   - Se loss=NaN: verifique normaliza√ß√£o dos dados (nodata, infinitos)
   - Se PSNR n√£o melhora: aumente epochs, ajuste lr, ou verifique dados
"""